{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Chatbot Tutorial \n",
    "This codelab provides an introduction to using Memorystore for Redis and LangChain. It walks through how to connect to and use Memorystore for Redis as a vector store, document loader, and chat history store. The codelab also provides a dataset of movie titles from Netflix that you can use to experiment with the tools.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googleapis/langchain-google-memorystore-redis-python/blob/main/samples/langchain_quick_start.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download the Netflix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Initialize the Google Cloud Storage client\n",
    "gcs_client = storage.Client()\n",
    "\n",
    "bucket_name = \"cloud-samples-data\"\n",
    "source_blob_name = \"langchain/netflix_titles_compute_embeddings.csv\"\n",
    "destination_file_name = \"./netflix_titles_compute_embeddings.csv\"\n",
    "\n",
    "# Get the bucket and blob (file) from GCS\n",
    "bucket = gcs_client.bucket(bucket_name)\n",
    "blob = bucket.blob(source_blob_name)\n",
    "\n",
    "# Download the file to the local destination\n",
    "blob.download_to_filename(destination_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data as LangChain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"./netflix_titles_compute_embeddings.csv\"\n",
    "\n",
    "# Initialize a list to hold the Document objects\n",
    "docs = []\n",
    "\n",
    "# Determine metadata fields by reading the CSV headers\n",
    "with open(csv_file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader, None)\n",
    "    # Exclude 'description' from metadata fields\n",
    "    metadata_fields = [header for header in headers if header != \"description\"]\n",
    "\n",
    "# Read the CSV file and construct Document objects\n",
    "with open(csv_file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Construct the content to include the entire row\n",
    "        content = \" | \".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "        # Construct metadata by excluding 'description' field\n",
    "        metadata = {field: row[field] for field in metadata_fields}\n",
    "        # Create the Document object with the entire row as content and the rest as metadata\n",
    "        doc = Document(page_content=content, metadata=metadata)\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Initialize an Embeddings Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "PROJECT_ID = \"my_project_id\"  # @param {type:\"string\"}\n",
    "embeddings_service = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=f\"{PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up a Connection to a Memorystore for Redis Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "REDIS_HOST = \"127.0.0.1:6379\"  # @param {type:\"string\"}\n",
    "client = redis.from_url(f\"redis://{REDIS_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Vector Index in the Memorystore for Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_memorystore_redis import (\n",
    "    DistanceStrategy,\n",
    "    HNSWConfig,\n",
    "    RedisVectorStore,\n",
    ")\n",
    "\n",
    "index_config = HNSWConfig(\n",
    "    name=\"netflix_complete:\", distance_strategy=DistanceStrategy.COSINE, vector_size=768\n",
    ")\n",
    "\n",
    "RedisVectorStore.init_index(client=client, index_config=index_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = RedisVectorStore(\n",
    "    client=client, index_name=\"netflix_complete:\", embeddings=embeddings_service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Documents to the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Memorystore for Redis as `ChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_memorystore_redis import MemorystoreChatMessageHistory\n",
    "\n",
    "chat_history = MemorystoreChatMessageHistory(\n",
    "    client=client,\n",
    "    session_id=\"my_session\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Movie Question-Answering Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_memorystore_redis import MemorystoreChatMessageHistory\n",
    "\n",
    "# Suppress all deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Prepare some prompt templates for the ConversationalRetrievalChain\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Use all the information from the context and the conversation history to answer new question. If you see the answer in previous conversation history or the context. \\\n",
    "Answer it with clarifying the source information. If you don't see it in the context or the chat history, just say you \\\n",
    "didn't find the answer in the given data. Don't make things up.\n",
    "\n",
    "Previous conversation history from the questioner. \"Human\" was the user who's asking the new question. \"Assistant\" was you as the assistant:\n",
    "```{chat_history}\n",
    "```\n",
    "\n",
    "Vector search result of the new question:\n",
    "```{context}\n",
    "```\n",
    "\n",
    "New Question:\n",
    "```{question}```\n",
    "\n",
    "Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    ")\n",
    "condense_question_prompt_passthrough = PromptTemplate(\n",
    "    template=\"\"\"Repeat the following question:\n",
    "{question}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Intialize retriever, llm and memory for the chain\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 5, \"lambda_mult\": 0.8}\n",
    ")\n",
    "llm = VertexAI(model_name=\"gemini-pro\", project=f\"{PROJECT_ID}\")\n",
    "\n",
    "chat_history.clear()\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    chat_memory=chat_history,\n",
    "    output_key=\"answer\",\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# create the ConversationalRetrievalChain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=condense_question_prompt_passthrough,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Your Chatbot Movie Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask some questions\n",
    "q = \"What movie was Brad Pitt in?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")\n",
    "\n",
    "q = \"How about Jonny Depp?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")\n",
    "\n",
    "q = \"Are there movies about animals?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
